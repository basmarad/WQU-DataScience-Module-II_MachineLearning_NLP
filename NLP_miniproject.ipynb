{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The objective of this miniproject is to gain experience with natural language processing and how to use text data to train a machine learning model to make predictions. For the miniproject, we will be working with product review text from Amazon. The reviews are for only products in the \"Electronics\" category. The objective is to train a model to predict the rating, ranging from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading the data\n",
    "\n",
    "The data set is available on Amazon S3 and comes as a compressed file where each line is a JSON object. To load the data set, we  need to use the `gzip` library to open the file and decode each JSON into a Python dictionary. In the end, we have a list of dictionaries, where each dictionary represents an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data\n",
    "\n",
    "os.system('/usr/local/bin/wget \"http://dataincubator-wqu.s3.amazonaws.com/mldata/amazon_electronics_reviews_training.json.gz\" -nc -P ./nlp-data/')# had to install wget first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "os.chdir('/Users/bmr225/Documents/WorldQuantUniversity')\n",
    "with gzip.open(\"./nlp-data/amazon_electronics_reviews_training.json.gz\", \"r\") as f:                                  \n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I bought this mouse to use with my laptop because I don't like those little touchpads.  I could not be happier.Since it's USB, I can plug it in with the computer already on and expect it to work automatically.  Since it's optical (the new kind, not to be confused with the old Sun optical mice that required a special checkered mouse pad) it works on most surfaces, including my pant legs, my couch, and random tables that I put my laptop down on.  It's also light and durable, features that help with portability.The wheel is surprisingly useful.  In addition to scrolling, it controls zoom and pan in programs like Autocad and 3D Studio Max.  I can no longer bear using either of these programs without it.One complaint - the software included with the Internet navigation features is useless.  Don't bother installing it if you have a newer Windows version that automatically supports wheel mice.  Just plug it in and use it - it's that easy.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample observation\n",
    "data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a custom transformer to extract the value of the key reviewText\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ExtractKey(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    def transform(self,X):  \n",
    "        return [X[i][self.key] for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would have to include Spacy first\n",
    "import spacy\n",
    "# load text processing pipeline\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the stopwords\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "STOP_WORDS_rev = STOP_WORDS.union({'mouse','software','computer','plug','USB','pad','touchpad','pad','cable','Floppy Disk Drive','install'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [data[i]['overall'] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between CountVectorizer and tfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r2: 0.42811605136161623\n"
     ]
    }
   ],
   "source": [
    "# I. CountVectorizer\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "Extract_reviewtext = ExtractKey('reviewText')\n",
    "tdidf = TfidfVectorizer(stop_words=STOP_WORDS_rev,ngram_range=(1,1),tokenizer = None)\n",
    "countvectorizer = CountVectorizer()\n",
    "ridge_regressor = Ridge(alpha=1, solver ='sag')\n",
    "\n",
    "\n",
    "bag_of_words_model = Pipeline([('extract_reviews', Extract_reviewtext), ('vectorizer',countvectorizer), ('regressor', ridge_regressor)])\n",
    "bag_of_words_model.fit(data, ratings)\n",
    "\n",
    "print(\"Model r2: {}\".format(bag_of_words_model.score(data, ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmr225/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['disk', 'drive', 'floppy', 'll', 'usb', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/bmr225/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:558: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  '\"sag\" solver requires many iterations to fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r2: 0.5895969413827675\n"
     ]
    }
   ],
   "source": [
    "#II. Normalized Model: TfidVectorizer\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "Extract_reviewtext = ExtractKey('reviewText')\n",
    "tdidf = TfidfVectorizer(stop_words=STOP_WORDS_rev,tokenizer = None)\n",
    "countvectorizer = CountVectorizer()\n",
    "ridge_regressor = Ridge(alpha=1, solver ='sag')\n",
    "\n",
    "\n",
    "normalized_model = Pipeline([('extract_reviews', Extract_reviewtext), ('vectorizer',tdidf), ('regressor', ridge_regressor)])\n",
    "normalized_model.fit(data, ratings)\n",
    "\n",
    "print(\"Model r2: {}\".format(normalized_model.score(data, ratings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model improved with the normalized count\n",
    "\n",
    "The model performance may increase when including additional features generated by counting bigrams. Include bigrams to your model. When using more features, the risk of overfitting increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Bigrams and Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed: 1167.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2:0.32678109133473887\n",
      "CV R^2:0.2974161548844446\n",
      "Test R^2:0.2908774906523537\n"
     ]
    }
   ],
   "source": [
    "# To hide all the warnings in Python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "# To check for overfitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,ratings, test_size = 0.2, random_state = 0)\n",
    "\n",
    "Extract_reviewtext = ExtractKey('reviewText')\n",
    "tdidf = TfidfVectorizer(stop_words=STOP_WORDS_rev)\n",
    "countvectorizer = CountVectorizer()\n",
    "ridge_regressor = Ridge(solver ='sag')\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "pipe = Pipeline([('extract_reviews', Extract_reviewtext), ('vectorizer',tdidf), ('regressor', ridge_regressor)])\n",
    "#pipe.fit(data, ratings)\n",
    "\n",
    "\n",
    "param_grid = {'vectorizer__ngram_range':[(1,1),(1,2),(2,2)],\n",
    "              'vectorizer__tokenizer':[None,lemmatizer],\n",
    "              'regressor__alpha':np.logspace(-8,1)} # for now let's do like the teacher.\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 5, verbose = 1)\n",
    "grid_search.fit(X_train[:3000],y_train[:3000]) #Using part of the data for tuning to speed up the process\n",
    "\n",
    "\n",
    "print('Training R^2:{}'.format(grid_search.score(X_train,y_train)))\n",
    "print('CV R^2:{}'.format(grid_search.best_score_)) \n",
    "print('Test R^2:{}'.format(grid_search.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__alpha': 0.22229964825261955,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__tokenizer': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final model: stop_words, bigram, tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('extract_reviews', ExtractKey(key='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words={\"'d...\n",
       "                                             'also', 'although', 'always', 'am',\n",
       "                                             'among', 'amongst', 'amount', 'an',\n",
       "                                             'and', ...},\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('regressor',\n",
       "                 Ridge(alpha=0.2222996482, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='sag', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model with hypertuned parameters\n",
    "from sklearn.linear_model import Ridge\n",
    "Extract_reviewtext = ExtractKey('reviewText')\n",
    "tdidf = TfidfVectorizer(stop_words=STOP_WORDS_rev,ngram_range=(1,2),tokenizer = None)\n",
    "ridge_regressor = Ridge(alpha=0.2222996482, solver = 'sag')\n",
    "\n",
    "\n",
    "bigrams_model = Pipeline([('extract_reviews', Extract_reviewtext), ('vectorizer',tdidf), ('regressor', ridge_regressor)])\n",
    "bigrams_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r2: 0.9690501453968942\n",
      "Test r2: 0.4406043971082288\n"
     ]
    }
   ],
   "source": [
    "print(\"Training r2: {}\".format(bigrams_model.score(X_train, y_train)))\n",
    "print(\"Test r2: {}\".format(bigrams_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly there is an overfitting problem.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation II\n",
    "\n",
    "## Polarity analysis\n",
    "Let's derive some insight from our analysis. We want to determine the most polarizing words in the corpus of reviews. In other words, we want identify words that strongly signal a review is either positive or negative. For example, we understand a word like \"terrible\" will mostly appear in negative rather than positive reviews. The naive Bayes model calculates probabilities such as  $P(\\text{terrible } | \\text{ negative})$, the probability the review is negative given the word \"terrible\" appears in the text. Using these probabilities, we can derive a polarity score for each counted word,\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{polarity} =  \\log\\left(\\frac{P(\\text{word } | \\text{ positive})}{P(\\text{word } | \\text{ negative})}\\right).\n",
    "$$ \n",
    "\n",
    " \n",
    "The polarity analysis is an example where a simpler model offers more explicability than a more complicated model. For this question, you are asked to determine the top twenty-five words with the largest positive and largest negative polarity, for a total of fifty words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, ratings # empty the memory from first data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create data set and labels\n",
    "with gzip.open('./nlp-data/amazon_one_and_five_star_reviews.json.gz', \"r\") as f:\n",
    "    data_polarity = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [data_polarity[i]['overall'] for i in range(len(data_polarity))]\n",
    "reviews = [data_polarity[i]['reviewText'] for i in range(len(data_polarity))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmr225/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9085\n"
     ]
    }
   ],
   "source": [
    "# This is bad as there is no pipeline\n",
    "# TfidVectorizer fit_transform\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=STOP_WORDS) \n",
    "#tfidf = TfidfVectorizer(stop_words=STOP_WORDS,tokenizer=lemmatizer, ngram_range=(1, 1)) # This gave a lot of punctuation\n",
    "# lemmatizer fails now.. \n",
    "reviews_tfdif = tfidf.fit_transform(reviews)\n",
    "\n",
    "# create and train pipeline\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tfdif, ratings , test_size = 0.2, random_state = 0)\n",
    "\n",
    "#Train the classifier\n",
    "class_model = MultinomialNB()\n",
    "class_model.fit(X_train,y_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "\n",
    "y_proba = class_model.predict_proba(X_train)\n",
    "\n",
    "predicted_indices =[1 if y_proba[i,0]>0.5 else 5 for i in range(len(y_proba))] \n",
    "\n",
    "print(\"Training accuracy: {}\".format(class_model.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9085\n",
      "accuracy: 0.9085\n",
      "Multinomial naive bayes AUC: 0.0916058960783564\n",
      "Test accuracy: 0.849\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Training accuracy: {}\".format(class_model.score(X_train, y_train)))\n",
    "print(\"accuracy: {}\".format(metrics.accuracy_score(y_train, predicted_indices))) # Same thing.\n",
    "\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, predicted_indices, pos_label=1)\n",
    "print(\"Multinomial naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))# is this very low... how can I fix that.. \n",
    "print(\"Test accuracy: {}\".format(class_model.score(X_test, y_test))) # overfitting happening here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now fitting the model on the whole data set\n",
    "class_model = MultinomialNB()\n",
    "class_model.fit(reviews_tfdif,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving Polarity\n",
    "negative_prob = class_model.feature_log_prob_[0] # both formats work \n",
    "positive_prob = class_model.feature_log_prob_[1,:]\n",
    "\n",
    "# Computing the log of the ratios of probabilities \n",
    "post_pos_ratio = positive_prob - negative_prob  # these are log of the ratio of prob\n",
    "post_neg_ratio = negative_prob - positive_prob\n",
    "\n",
    "\n",
    "## Sorting the log of the ratios\n",
    "sorted_post_pos_ratio_ind = post_pos_ratio.argsort() # usually in ascending.. \n",
    "# For positive Class\n",
    "sorted_post_neg_ratio_ind = post_neg_ratio.argsort()\n",
    "# list of features\n",
    "features_l = list(tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['highly', 'beat', 'protects', 'perfect', 'monopod', 'portrait', 'amazing', 'sturdy', 'macro', 'incredible', 'excellent', 'bokeh', 'pleased', '200mm', 'charm', 'handy', 'awesome', 'portraits', 'dslr', 'crisp', 'photography', 'telephoto', 'buck', 'fantastic', 'regrets']\n"
     ]
    }
   ],
   "source": [
    "print([features_l[ind] for ind in sorted_post_pos_ratio_ind[:-26:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['refund', 'waste', 'return', 'returning', 'worst', 'junk', 'terrible', 'returned', 'garbage', 'useless', 'worthless', 'worse', 'trash', 'defective', 'beware', 'poor', 'unacceptable', 'awful', 'horrible', 'unreliable', 'stopped', 'randomly', 'disappointing', 'threw', 'refused']\n"
     ]
    }
   ],
   "source": [
    "print([features_l[ind] for ind in sorted_post_neg_ratio_ind[:-26:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 50 words: 25 top positive and 25 top negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos_25 = [features_l[ind] for ind in sorted_post_pos_ratio_ind[:-26:-1]]\n",
    "top_neg_25 = [features_l[ind] for ind in sorted_post_neg_ratio_ind[:-26:-1]]\n",
    "top_50 = top_pos_25 + top_neg_25 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling \n",
    "\n",
    "\n",
    "Topic modeling is the analysis of determining the key topics or themes in a corpus. With respect to machine learning, topic modeling is an unsupervised technique. One way to uncover the main topics in a corpus is to use non-negative matrix factorization. For this question, use non-negative matrix factorization to determine the top ten words for the first twenty topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "working got amazon months money don buy bought unit time \n",
      "\n",
      "Topic: 1\n",
      "zoom wide nikon sharp light hood 50mm focus lenses canon \n",
      "\n",
      "Topic: 2\n",
      "volume music headphone comfortable head bass pair ears ear sound \n",
      "\n",
      "Topic: 3\n",
      "video needed length extension modem belkin signal connect monitor tv \n",
      "\n",
      "Topic: 4\n",
      "picture remote flash battery batteries canon cameras use digital pictures \n",
      "\n",
      "Topic: 5\n",
      "players dvds tv unit discs disc sony play cd player \n",
      "\n",
      "Topic: 6\n",
      "bought perfect use value need worked highly easy recommend price \n",
      "\n",
      "Topic: 7\n",
      "button hand mice used buttons microsoft trackball use logitech keyboard \n",
      "\n",
      "Topic: 8\n",
      "excellent products tech advertised service purchase does support amazon recommend \n",
      "\n",
      "Topic: 9\n",
      "better volume set bose surround sub bass wire speaker sound \n",
      "\n",
      "Topic: 10\n",
      "sd drivers software quot sandisk cf install memory windows cards \n",
      "\n",
      "Topic: 11\n",
      "tech link firmware internet support connection network netgear linksys wireless \n",
      "\n",
      "Topic: 12\n",
      "expensive protection quality lenses protect hoya tiffen glass uv filters \n",
      "\n",
      "Topic: 13\n",
      "high far say really pretty build value sound price quality \n",
      "\n",
      "Topic: 14\n",
      "devices port use plug adapter hub device computer drive power \n",
      "\n",
      "Topic: 15\n",
      "static car radios better station signal stations reception fm antenna \n",
      "\n",
      "Topic: 16\n",
      "needed expected charm say perfectly tv cord just like fine \n",
      "\n",
      "Topic: 17\n",
      "laptop like nice fits tripod strap small carry fit case \n",
      "\n",
      "Topic: 18\n",
      "support software windows remote make did doesn didn tried does \n",
      "\n",
      "Topic: 19\n",
      "ethernet length belkin ve cheap connectors monster ordered hosa quality \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLP example: code adapted from course resources\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "data = reviews\n",
    "\n",
    "n_topics = 20\n",
    "n_top_words = 10\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "nmf = NMF(n_components=n_topics, random_state=0)\n",
    "pipe = Pipeline([('vectorizer', tfidf), ('dim-red', nmf)])\n",
    "pipe.fit(data)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "for i, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic: {}\".format(i))\n",
    "    indices = topic.argsort()[-n_top_words-1:-1]\n",
    "    top_words = [feature_names[ind] for ind in indices]\n",
    "    print(\" \".join(top_words), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
